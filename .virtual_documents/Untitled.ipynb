from __future__ import print_function
import os
import cv2
import numpy as np
import tensorflow as tf
from deepface import DeepFace
import json
from utils.utils import Logger, mkdir
from utils import label_map_util
from detect_face import detect_face_by_frame

os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

logger = Logger()

# TensorFlow face detection model loading
PATH_TO_CKPT = './model/frozen_inference_graph_face.pb'
PATH_TO_LABELS = './protos/face_label_map.pbtxt'
NUM_CLASSES = 2

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

detection_graph = tf.Graph()

with detection_graph.as_default():
    od_graph_def = tf.compat.v1.GraphDef()
    with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(graph=detection_graph, config=config)



def capture_frames_from_video(video_path, output_dir, frame_interval=3, scale_rate=0.9):
    mkdir(output_dir)
    results = []

    video_name = os.path.basename(video_path)
    directoryname = os.path.join(output_dir, video_name.split('.')[0])
    mkdir(directoryname)
    logger.info('Video_name:{}'.format(video_path))
    cam = cv2.VideoCapture(video_path)
    c = 0

    while True:
        ret, frame = cam.read()
        if not ret:
            logger.warning("ret false")
            break
        if frame is None:
            logger.warning("frame drop")
            break

        frame = cv2.resize(frame, (0, 0), fx=scale_rate, fy=scale_rate)
        r_g_b_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        if c % frame_interval == 0:
            img_size = np.asarray(frame.shape)[0:2]
            faces, scores, classes, num_detections = detect_face_by_frame(detection_graph, session, r_g_b_frame)

            if faces.shape[0] > 0:
                best_face_path = os.path.join(directoryname, f"frame_{c}.jpg")
                cv2.imwrite(best_face_path, frame, [int(cv2.IMWRITE_JPEG_QUALITY), 95])
                logger.info(f"Saved {best_face_path}")

                results.append({
                    "video": video_path,
                    "captured_frame": os.path.abspath(best_face_path)
                })
                break  # Stop after processing the first detected face

        c += 1

    cam.release()
    return results

def analyze_frames(frames):
    analyzed_results = []
    for frame_info in frames:
        img_path = frame_info['captured_frame']
        print(f"Analyzing {img_path} from {frame_info['video']}")  # Debug print
        try:
            demography_objs = DeepFace.analyze(img_path=img_path, actions=['age', 'gender', 'race', 'emotion'], enforce_detection=False, silent=True)
            demography = demography_objs[0] if isinstance(demography_objs, list) else demography_objs
            print(f"Demography: {demography}")
            age = demography["age"]
            gender = demography["dominant_gender"]
            race = demography["dominant_race"]
            emotion = demography["dominant_emotion"]
            print(f"Age: {age}, Gender: {gender}, Race: {race}, Emotion: {emotion}")

            analyzed_results.append({
                "video": frame_info['video'],
                "captured_frame": img_path,
                "age": demography['age'],
                "dominant_gender": demography['dominant_gender'],
                "dominant_race": demography['dominant_race'],
                "dominant_emotion": demography['dominant_emotion']
            })
        except Exception as e:
            print(f"Error analyzing {img_path}: {e}")  # Debug print

    return analyzed_results

def save_results(results, output_path):
    with open(os.path.join(output_path, 'results.json'), 'w') as f:
        json.dump(results, f, indent=4)



# Define paths
root_dir = 'media'  # Change this to your directory
output_path = 'facepics'  # Change this to your output directory

# Ensure output directory exists
mkdir(output_path)

# Process each video in the root directory
all_results = []
for filename in os.listdir(root_dir):
    if filename.endswith(('.mp4', '.avi', '.mov', '.mkv')):
        video_path = os.path.join(root_dir, filename)
        frames = capture_frames_from_video(video_path, output_path)
        all_results.extend(frames)

# Analyze captured frames
analyzed_results = analyze_frames(all_results)

# Save results to JSON
save_results(analyzed_results, output_path)



img1=cv2.imread('facepics/0017_fake/frame_0.jpg')


demography = DeepFace.analyze(img_path=img1, detector_backend='yolov8', align='True')


from deepface import DeepFace
##Load and display input image
import cv2
import matplotlib.pyplot as plt
img1=cv2.imread("facepics/0017_fake/frame_0.jpg")
plt.imshow(img1)
plt.show()

plt.imshow(img1[:, :, ::-1 ])
plt.show()

obj=DeepFace.analyze(img1, actions = ['age', 'gender', 'race', 'emotion'])
print(obj)

print(type(obj))

final_obj={}
for i in obj:
    final_obj.update(i)
print (final_obj)
print(type(final_obj))

print(range(len(final_obj)))
print(final_obj['age'])
print(final_obj['dominant_gender'])
print(final_obj['dominant_race'])
print(final_obj['dominant_emotion'])

print("Your age is",final_obj['age'],"and you are",final_obj['dominant_race'],
      "and current emotion is",final_obj['dominant_emotion'],
     "and your gender capture is", final_obj['dominant_gender'])


!pip install onnxruntime onnxruntime-gpu


import cv2 
from utils import FaceAnalysis 
from matplotlib import pyplot as plt 
face_analyis_models = [
    "age",
    "gender",
    "race",
    "emotion"
]

age             = FaceAnalysis(model_name=face_analyis_models[0])
race            = FaceAnalysis(model_name=face_analyis_models[2])
emotion         = FaceAnalysis(model_name=face_analyis_models[-1])
face            = cv2.imread("facepics/0017_fake/frame_0.jpg")
age_output      = age.predict(input=face)
emotion_output  = emotion.predict(input=face)
plt.subplot(1,2,1)
plt.title(f"Emotion:{emotion_output}")
plt.imshow(face[...,::-1])
plt.subplot(1,2,2)
plt.title(f"Age:{age_output}")
plt.imshow(face[...,::-1])
plt.show()
print(race)



